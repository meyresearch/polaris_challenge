{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 80411,
     "status": "ok",
     "timestamp": 1741635881652,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "GxyOIutNB-tw",
    "outputId": "6cf756c1-8234-44fe-97a1-b78f14ad2540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting spacecutter\n",
      "  Downloading spacecutter-0.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Collecting datamol\n",
      "  Downloading datamol-0.12.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting skorch\n",
      "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from datamol) (4.67.1)\n",
      "Collecting loguru (from datamol)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from datamol) (1.4.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from datamol) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from datamol) (3.10.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from datamol) (11.1.0)\n",
      "Collecting selfies (from datamol)\n",
      "  Downloading selfies-2.2.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from datamol) (4.3.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from datamol) (1.6.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datamol) (24.2)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from datamol) (6.5.2)\n",
      "Collecting rdkit (from datamol)\n",
      "  Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->datamol) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (3.2.1)\n",
      "Downloading spacecutter-0.2.1-py3-none-any.whl (6.6 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datamol-0.12.5-py3-none-any.whl (495 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading selfies-2.2.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: selfies, rdkit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, nvidia-cusparse-cu12, nvidia-cudnn-cu12, skorch, nvidia-cusolver-cu12, datamol, spacecutter\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed datamol-0.12.5 loguru-0.7.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdkit-2024.9.5 selfies-2.2.0 skorch-1.1.0 spacecutter-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy spacecutter torch datamol skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21856,
     "status": "ok",
     "timestamp": 1741635903516,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "BO0oUhZsCOR1",
    "outputId": "9ac53fd1-fe0c-464a-b867-cdbaa029f8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1741635906432,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "PrX_hqdKD8H7"
   },
   "outputs": [],
   "source": [
    "proj_dir =  'drive/MyDrive/Polaris_ASAP_competition/polaris_challenge/admet'\n",
    "# proj_dir = '/Users/robertarbon/Library/CloudStorage/GoogleDrive-robert.arbon@gmail.com/My Drive/Polaris_ASAP_competition/polaris_challenge/admet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4234,
     "status": "ok",
     "timestamp": 1741635912487,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "FamOIzIxB9TS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacecutter.models import OrdinalLogisticModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import datamol as dm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import SkorchDoctor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from spacecutter.callbacks import AscensionCallback\n",
    "from spacecutter.losses import CumulativeLinkLoss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# from molfeat.trans.pretrained.hf_transformers import PretrainedHFTransformer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV1UjUAUCOCf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5268,
     "status": "ok",
     "timestamp": 1741635917757,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "7rgAx5oEB9TW"
   },
   "outputs": [],
   "source": [
    "# Imputed training data\n",
    "df_imp = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_log_pmm_imputed.csv')\n",
    "# Non-imputed validation data\n",
    "df_val = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_features.csv')\n",
    "# change names\n",
    "df_val.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "df_imp.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "# Smiles columns because they were removed (for some unknown reason)\n",
    "df_smiles = pd.read_csv(f'{proj_dir}/data/train_admet_all.csv')\n",
    "df_smiles.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "\n",
    "df_imp = df_imp.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')\n",
    "df_val = df_val.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1741636483590,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "JRfXbaZGB9TY",
    "outputId": "55e3c7e8-19eb-4f75-96b0-cd9d96e499f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "using chemberta\n",
      "\tcreating new scaler\n",
      "using chemprop features\n",
      "\tcreating new scaler\n",
      "using rdkit simple\n",
      "\tcreating new scaler\n",
      "validation data\n",
      "using chemberta\n",
      "\tusing existing scaler\n",
      "using chemprop features\n",
      "\tusing existing scaler\n",
      "using rdkit simple\n",
      "\tusing existing scaler\n",
      "LogD\n",
      "\t0/1,\tmae: 0.36\n",
      "\tmean epoches: 29.0\n",
      "LogMLM\n",
      "\t0/1,\tmae: 0.46\n",
      "\tmean epoches: 1.0\n",
      "LogHLM\n",
      "\t0/1,\tmae: 0.67\n",
      "\tmean epoches: 30.0\n",
      "LogKSOL\n",
      "\t0/1,\tmae: 0.53\n",
      "\tmean epoches: 2.0\n",
      "LogMDR1.MDCKII\n",
      "\t0/1,\tmae: 0.64\n",
      "\tmean epoches: 2.0\n"
     ]
    }
   ],
   "source": [
    "train, val = train_data(df_train=df_imp,\n",
    "                        imp_ix=1,\n",
    "                        df_val=df_val,\n",
    "                        n_cuts=None, features=['chemberta', 'chem_prop', 'rdkit_simple'])\n",
    "\n",
    "targets = list(train[1].keys())\n",
    "\n",
    "training_by_target = {}\n",
    "\n",
    "for target in targets:\n",
    "  num_ds = 1\n",
    "  ix_by_imp = train[2]\n",
    "  if ix_by_imp is not None:\n",
    "    num_ds = len(ix_by_imp)\n",
    "\n",
    "  # accumulators\n",
    "  all_y_hat = []\n",
    "  all_epochs = []\n",
    "\n",
    "  # All imputed training datasets\n",
    "  all_X = train[0]\n",
    "  all_y = train[1][target]['values'].reshape(-1, 1)\n",
    "\n",
    "  # only keep features which are different\n",
    "  keep_ix = np.std(all_X, axis=0)>0\n",
    "  all_X = all_X[:, keep_ix].astype(np.float32)\n",
    "\n",
    "  # Get validation data (not imputed so only done once. contains missing values)\n",
    "  missing_ix = val[1][target]['missing_ix']\n",
    "  Xval = val[0]\n",
    "\n",
    "  Xval = Xval[missing_ix, :].astype(np.float32)\n",
    "  Xval = Xval[:, keep_ix]\n",
    "  # validation y values have been digitized using all the training y values.\n",
    "  # so should be consistent.\n",
    "  yval = val[1][target]['values'].reshape(-1, 1)\n",
    "  bins = train[1][target]['bins']\n",
    "\n",
    "  print(f'{target}')\n",
    "  for i in range(num_ds):\n",
    "    if i % 10 == 0:\n",
    "      print(f'\\t{i}/{num_ds}', end=',')\n",
    "\n",
    "    # Get imputed training dataset.\n",
    "    imp_ix = ix_by_imp[i+1] if ix_by_imp is not None else np.arange(all_X.shape[0]) # The zeroth imputed dataset is the original data.\n",
    "    X = all_X[imp_ix, :]\n",
    "    y = train[1][target]['values'].reshape(-1, 1)\n",
    "    y = y[imp_ix]\n",
    "\n",
    "    # print(f\"{target}:\\n\\tn_train_obs: {X.shape[0]}, n_val_obs: {Xval.shape[0]} n_preds: {X.shape[1]}\")\n",
    "\n",
    "    # Stack all data for convenience.\n",
    "    train_v_X = np.vstack([X, Xval])\n",
    "    train_v_y = np.vstack([y, yval])\n",
    "    train_ix = np.arange(X.shape[0])\n",
    "    val_ix = np.arange(X.shape[0], train_v_X.shape[0])\n",
    "\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "\n",
    "    predictor = nn.Sequential(\n",
    "        nn.Linear(num_features, num_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features, num_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "    skorch_model = NeuralNet(\n",
    "        module=OrdinalLogisticModel,\n",
    "        module__predictor=predictor,\n",
    "        module__num_classes=num_classes,\n",
    "        criterion=CumulativeLinkLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        train_split=lambda ds, y: (torch.utils.data.Subset(ds, train_ix),\n",
    "                                  torch.utils.data.Subset(ds, val_ix)),\n",
    "        callbacks=[\n",
    "            ('ascension', AscensionCallback()),\n",
    "            ('early_stopping', EarlyStopping(threshold=0.0001, load_best=True,\n",
    "                                            patience=100))\n",
    "        ],\n",
    "        verbose=0,\n",
    "        batch_size=X.shape[0],\n",
    "        max_epochs=500,\n",
    "\n",
    "    )\n",
    "\n",
    "    skorch_model.fit(train_v_X, train_v_y)\n",
    "\n",
    "\n",
    "    y_hat = np.argmax(skorch_model.predict(Xval), axis=1)\n",
    "\n",
    "\n",
    "    all_y_hat.append(bins[y_hat])\n",
    "\n",
    "    for i in range(len(skorch_model.history)-1, -1, -1):\n",
    "      batch = skorch_model.history[i]\n",
    "\n",
    "      if batch['valid_loss_best']:\n",
    "        all_epochs.append(batch['epoch'])\n",
    "        break\n",
    "\n",
    "  # mean prediction over imputed datasets.\n",
    "  mean_y_hat = np.mean(np.vstack(all_y_hat), axis=0)\n",
    "  y_val_cont = bins[yval.reshape(-1)]\n",
    "\n",
    "  print(f\"\\tmae: {mean_absolute_error(y_val_cont, mean_y_hat):4.2f}\")\n",
    "  print(f\"\\tmean epoches: {np.mean(all_epochs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309924,
     "status": "ok",
     "timestamp": 1741622532708,
     "user": {
      "displayName": "Rob Arbon",
      "userId": "03890706039979050860"
     },
     "user_tz": 0
    },
    "id": "JHRuE3q9niVs",
    "outputId": "8f396da6-c914-4181-c104-dee4ce321bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using rdkit simple\n",
      "using chemprop features\n",
      "using rdkit simple\n",
      "using chemprop features\n",
      "\tmae: 0.61\n",
      "\tepochs: 12/112\n",
      "\tmae: 0.45\n",
      "\tepochs: 6/106\n",
      "\tmae: 0.51\n",
      "\tepochs: 33/133\n",
      "\tmae: 0.59\n",
      "\tepochs: 20/120\n",
      "\tmae: 0.87\n",
      "\tepochs: 23/123\n"
     ]
    }
   ],
   "source": [
    "# train, val = train_data(df_train=df_imp,\n",
    "#                         imp_ix=None,\n",
    "#                         df_val=df_val,\n",
    "#                         n_cuts=None, features=['rdkit_simple', 'chemp_prop'])\n",
    "\n",
    "# targets = list(train[1].keys())\n",
    "\n",
    "# training_by_target = {}\n",
    "# n_imp_ds = df_imp['.imp'].max()\n",
    "\n",
    "# for target in targets:\n",
    "#   all_X = train[0]\n",
    "#   all_y = train[1][target]['values'].reshape(-1, 1)\n",
    "\n",
    "#   # only keep features which are different\n",
    "#   keep_ix = np.std(all_X, axis=0)>0\n",
    "#   all_X = all_X[:, keep_ix].astype(np.float32)\n",
    "\n",
    "#   # Get imputed training dataset.\n",
    "#   shuffle_ix = np.random.choice(np.arange(all_X.shape[0]), size=all_X.shape[0], replace=False)\n",
    "#   X = all_X[shuffle_ix, :]\n",
    "#   y = all_y[shuffle_ix]\n",
    "\n",
    "#   # Get validation data (not imputed so only done once. contains missing values)\n",
    "#   missing_ix = val[1][target]['missing_ix']\n",
    "#   Xval = val[0]\n",
    "#   Xval = Xval[missing_ix, :].astype(np.float32)\n",
    "#   Xval = Xval[:, keep_ix]\n",
    "#   # validation y values have been digitized using all the training y values.\n",
    "#   # so should be consistent.\n",
    "#   yval = val[1][target]['values'].reshape(-1, 1)\n",
    "#   bins = train[1][target]['bins']\n",
    "\n",
    "#   # batch size\n",
    "#   batch_size = all_X.shape[0]//n_imp_ds\n",
    "\n",
    "#   # Stack all data for convenience.\n",
    "#   train_v_X = np.vstack([X, Xval])\n",
    "#   train_v_y = np.vstack([y, yval])\n",
    "#   train_ix = np.arange(X.shape[0])\n",
    "#   val_ix = np.arange(X.shape[0], train_v_X.shape[0])\n",
    "\n",
    "#   # Model dimensions\n",
    "#   num_features = X.shape[1]\n",
    "#   num_classes = len(np.unique(y))\n",
    "\n",
    "#   # Simple predictor\n",
    "#   predictor = nn.Sequential(\n",
    "#     nn.Linear(num_features, num_features),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(num_features, num_features),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(num_features, 1),\n",
    "#   )\n",
    "\n",
    "#   # Model\n",
    "#   skorch_model = NeuralNet(\n",
    "#     module=OrdinalLogisticModel,\n",
    "#     module__predictor=predictor,\n",
    "#     module__num_classes=num_classes,\n",
    "#     criterion=CumulativeLinkLoss,\n",
    "#     optimizer=torch.optim.Adam,\n",
    "#     train_split=lambda ds, y: (torch.utils.data.Subset(ds, train_ix),\n",
    "#                               torch.utils.data.Subset(ds, val_ix)),\n",
    "#     callbacks=[\n",
    "#         ('ascension', AscensionCallback()),\n",
    "#         ('early_stopping', EarlyStopping(threshold=0.0001, load_best=True,\n",
    "#                                         patience=100))\n",
    "#     ],\n",
    "#     verbose=0,\n",
    "#     batch_size=X.shape[0],\n",
    "#     max_epochs=500,\n",
    "\n",
    "#   )\n",
    "#   # Fit\n",
    "#   skorch_model.fit(train_v_X, train_v_y)\n",
    "\n",
    "#   # predict on validation\n",
    "#   y_hat_ord = np.argmax(skorch_model.predict(Xval), axis=1)\n",
    "#   y_hat = bins[y_hat_ord]\n",
    "#   y_val = bins[yval.reshape(-1)]\n",
    "\n",
    "#   print(f\"\\tmae: {mean_absolute_error(y_val, y_hat):4.2f}\")\n",
    "\n",
    "#   # Find best epoch\n",
    "#   for i in range(len(skorch_model.history)-1, -1, -1):\n",
    "#     batch = skorch_model.history[i]\n",
    "\n",
    "#     if batch['valid_loss_best']:\n",
    "#       print(f\"\\tepochs: {batch['epoch']}/{len(skorch_model.history)}\")\n",
    "#       break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spacecutter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
