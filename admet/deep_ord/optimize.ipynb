{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import datamol as dm\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from spacecutter.models import OrdinalLogisticMultiTaskModel\n",
    "from spacecutter.losses import MultiTaskCumulativeLinkLoss\n",
    "from spacecutter.callbacks import AscensionCallback\n",
    "\n",
    "from utils import train_data, to_model_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/Users/robertarbon/Library/CloudStorage/GoogleDrive-robert.arbon@gmail.com/My Drive/Polaris_ASAP_competition/polaris_challenge/admet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputed training data\n",
    "df_imp = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_log_pmm_imputed.csv')\n",
    "# Non-imputed validation data\n",
    "df_val = pd.read_csv(f'{proj_dir}/dm_features/ordinal_data_split_2/train_admet_split2_features.csv')\n",
    "# change names\n",
    "df_val.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "df_imp.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "# Smiles columns because they were removed (for some unknown reason)\n",
    "df_smiles = pd.read_csv(f'{proj_dir}/data/train_admet_all.csv')\n",
    "df_smiles.rename(columns={'Molecule Name': 'Molecule.Name', 'LogMDR1-MDCKII':'LogMDR1.MDCKII'}, inplace=True)\n",
    "\n",
    "df_imp = df_imp.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')\n",
    "df_val = df_val.merge(df_smiles.loc[:, ['Molecule.Name', 'CXSMILES']], on='Molecule.Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 92038745\n",
    "features = ['chemberta', 'chem_prop']\n",
    "patience = 100\n",
    "storage_name = \"sqlite:///mtl_ordinal.db\"\n",
    "study_name = f\"{'_'.join(features)}_{patience}\"\n",
    "n_startup_trials = 20\n",
    "n_model_trials = 100\n",
    "n_total_trials = n_startup_trials + n_model_trials\n",
    "\n",
    "\n",
    "train, val = train_data(df_imp, imp_ix=1, df_val=df_val, n_cuts=None, features=features, proj_dir=proj_dir, remove_nans=False)\n",
    "X, y, train_ix, val_ix, config = to_model_format(train, val)\n",
    "\n",
    "def objective(trial):\n",
    "    weight_decay = trial.suggest_float(\"weight_deacy\", low=1e-6, high=1, log=True)\n",
    "    backbone_depth = trial.suggest_int(\"backbone_depth\", 1, 5)\n",
    "    head_depth = trial.suggest_int(\"head_depth\", 1, 3)\n",
    "\n",
    "    \n",
    "    n_features = config['n_features']\n",
    "    backbone = []\n",
    "    for i in range(backbone_depth):\n",
    "        backbone.append((f\"Backbone_FC_{i}\",nn.Linear(n_features, n_features)))\n",
    "        backbone.append((f\"Backbone_ReLU_{i}\", nn.ReLU()))\n",
    "    backbone = nn.Sequential(OrderedDict(backbone))\n",
    "\n",
    "    head = []\n",
    "    for i in range(head_depth):\n",
    "        if i < head_depth - 1: \n",
    "            out_dim = n_features\n",
    "        else:\n",
    "            out_dim = 1\n",
    "        head.append((f\"Head_FC_{i}\",nn.Linear(n_features, out_dim)))\n",
    "        head.append((f\"Head_ReLU_{i}\", nn.ReLU()))\n",
    "    head = nn.Sequential(OrderedDict(head))\n",
    "     \n",
    "    \n",
    "    # out_dim = max(n_features//10, 2)\n",
    "    model = NeuralNet(\n",
    "        module=OrdinalLogisticMultiTaskModel,\n",
    "        module__backbone=backbone,\n",
    "        module__head=head,\n",
    "        module__n_classes=config['n_classes_per_task'],\n",
    "        criterion=MultiTaskCumulativeLinkLoss,\n",
    "        criterion__n_tasks=config['n_tasks'],\n",
    "        criterion__n_classes_per_task = config['n_classes_per_task'], \n",
    "        criterion__loss_reduction = 'inv_num_classes', \n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__weight_decay = weight_decay,\n",
    "        train_split=lambda ds, y: (torch.utils.data.Subset(ds, train_ix),\n",
    "                                    torch.utils.data.Subset(ds, val_ix)),\n",
    "        callbacks=[\n",
    "            ('ascension', AscensionCallback()),\n",
    "            ('early_stopping', EarlyStopping(threshold=0.0001, load_best=True,\n",
    "                                            patience=patience))\n",
    "        ],\n",
    "        verbose=0,\n",
    "        batch_size=train_ix.shape[0],\n",
    "        max_epochs=1000,\n",
    "    )\n",
    "\n",
    "    model.fit(X, y)\n",
    "    # Get all predictions (train + val)\n",
    "    mod = model.module_\n",
    "    mod.eval()\n",
    "    y_pred_list = [x.cpu().detach().numpy() for x in mod.forward(torch.as_tensor(X))]\n",
    "    y_preds_ord = np.concatenate([np.argmax(x, axis=1).reshape(-1, 1) for x in y_pred_list], axis=1)\n",
    "\n",
    "    # Convert to continuous\n",
    "    y_pred_cont = []\n",
    "    for i, target in enumerate(config['targets']):\n",
    "        bins = train[1][target]['bins']\n",
    "        y_pred_cont.append(np.array([bins[x] if not np.isnan(x) else np.nan for x in y_preds_ord[:, i]]).reshape(-1, 1))\n",
    "    y_pred_cont = np.concatenate(y_pred_cont, axis=1)\n",
    "\n",
    "    y_true_train_cont = np.concatenate([train[1][targ]['original'].reshape(-1, 1) for targ in config['targets']], axis=1)\n",
    "    y_true_val_cont = np.concatenate([val[1][targ]['original'].reshape(-1, 1) for targ in config['targets']], axis=1)\n",
    "    y_true_cont = np.concatenate([y_true_train_cont, y_true_val_cont], axis=0)\n",
    "        \n",
    "    diff = np.abs(y_pred_cont - y_true_cont)\n",
    "\n",
    "    train_mask = np.isin(np.arange(diff.shape[0]), train_ix).reshape(-1, 1)\n",
    "    val_mask = np.isin(np.arange(diff.shape[0]), val_ix).reshape(-1, 1)\n",
    "    train_mae = np.mean(diff, where=~np.isnan(diff) & train_mask)\n",
    "    val_mae = np.mean(diff, where=~np.isnan(diff) & val_mask)\n",
    "    return val_mae, np.abs(train_mae-val_mae)\n",
    "\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=n_startup_trials, seed=random_state, multivariate=True)\n",
    "study = optuna.create_study(study_name=study_name, \n",
    "                            storage=storage_name, \n",
    "                            directions=['minimize', 'minimize'], \n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=n_total_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_optimization_history, plot_param_importances, plot_pareto_front, plot_slice\n",
    "\n",
    "# plot_pareto_front(study, include_dominated_trials=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_names = optuna.study.get_all_study_names(storage=\"sqlite:///mtl_ordinal.db\")\n",
    "all_trials = []\n",
    "for study_name in study_names:\n",
    "    study = optuna.study.load_study(storage=\"sqlite:///mtl_ordinal.db\", study_name=study_name)\n",
    "    df = study.trials_dataframe()\n",
    "    if study_name.endswith('_100'):\n",
    "        patience = 100\n",
    "        features = study_name[:-len('_100')]\n",
    "    else: \n",
    "        patience = 10\n",
    "        features = study_name\n",
    "    print(study_name, patience, features)\n",
    "    df.rename(columns={'values_0': 'MAE(val.)', 'values_1': '|MAE(val.) - MAE(train)|'}, inplace=True)\n",
    "    df['patience'] = patience\n",
    "    df['features'] = features\n",
    "    all_trials.append(df)\n",
    "df = pd.concat(all_trials)\n",
    "df.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# fig = px.scatter(df, x=\"MAE(val.)\", y=\"|MAE(val.) - MAE(train)|\", \n",
    "#                  color='features', symbol='patience', \n",
    "#                  custom_data=['params_backbone_depth', \n",
    "#                               'params_head_depth', \n",
    "#                               'params_weight_deacy', \n",
    "#                               'features'], \n",
    "#                  )\n",
    "# fig.update_traces(\n",
    "#     hovertemplate=\"<br>\".join([\n",
    "#         \"MAE: %{x:4.2f}\",\n",
    "#         \"Overfitting: %{y:4.2f}\",\n",
    "#         \"Features:%{customdata[3]}\",\n",
    "#         \"backbone: %{customdata[0]:d}\",\n",
    "#         \"head: %{customdata[1]:d}\",\n",
    "#         \"weight decay: %{customdata[2]:4.1e}\",\n",
    "#     ])\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimum parameters: \n",
    "\n",
    "- features = 'chem_prop' + 'chemberta'\n",
    "- weight decay = 9.9e-5\n",
    "- backbone depth, head depth = 1, 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacecutter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
